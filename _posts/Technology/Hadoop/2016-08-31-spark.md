---
layout: post
title: Spark
category: 技术
tags: Hadoop
keywords: Spark
---

## 前言 

## 分布式计算

《Spark快速大数据分析》讲到spark shell时提到：使用其它shell工具，你只能用单机的硬盘和内存来操作数据，而Spark Shell可用来与分布式存储在许多机器的内存或者硬盘上的数据进行交互，并且处理过程的分发由spark自动控制完成。

所以呢，一个分布式计算框架一般要做以下几件事：

1. 任务调度，启动进程，启动其它主机的进程，来运行任务，并感知它们
2. 定义进程的任务：接收、汇报（通用的） + 数据处理
3. 数据处理的中间结果，对于单机则一直在内存中，对于分布式则要在主机之间流转

大数据跟迭代的密切关系，大数据一般是大量重复schema的数据。

## spark和MR

说一些官话的话，MR的不足如下

1. 复杂的计算需要大量的job完成
2. 依赖关系是由开发者自己管理的
3. 没有整理逻辑
4. 中间结果只能放在HDFS中
5. 对于实时数据处理的支持不够

我的一个感觉是，MapReduce对数据的处理由一整套自己的流程，中间Map和Reduce逻辑留给开发人员。这从它留给开发人员的接口可以看出来，Map和Reduce是一个个类，它们是一个复杂执行流程的一环。

数据处理的中间结果，因为要在主机之间流转，hadoop的选择是，将其搞成文件，而内存数据搞成文件后，通常又以行来划分。

而RDD无需文件来存储中间结果，所以hadoop操作和RDD有所不同，RDD的形式可以更丰富。

||存储|提供操作|
|---|----|----|
|mapreduce|hdfs|map/reduce|
|sparck|RDD|transform,actiion|

spark 更像是用户事先定义好：rdd从哪里来，到哪里去，数据如何变化。有的书中提到rdd之间会维护一个lineage graph。我觉得spark会对这个图进行类似sql语义的解析，然后**融合到数据的读取、转换和处理流程**。具体的说，数据库对外提供的抽象（或使用方式）是sql，在从物件文件读取数据前，先对sql进行完整的解析，先读哪，再读哪，是否有缓存，是否查索引，最终制定一个执行计划。spark对外的抽象是rdd代码，最终转化为rdd lineage graph，读取数据的时候，就可以决定哪一步可以过滤掉。sql可以向数据库读写数据，rdd也可以。而hadoop，数据的处理和数据的读写、中转是不相关的。因为是过程式的，哪怕后面的逻辑表明一半数据是废掉的，读取时依然要先全部读取到内存。

## spark 和strorm

storm做的是流式处理，就是数据不停地来，storm一直在运行，就是kafka、rabbit可以不停地接消息，storm可以不停地处理消息一样。

hadoop和spark等则是批处理，其数据源是一个明确的文件，输出也是一个名明确的文件。

（待整理）实时性和大数据量其实是不可兼得的，比如数据量很大的话，就要移动计算（而不是移动数据），这时实现实时性就比较困难。


## spark 和marathon

mesos上，提供framework的抽象，可以运行很多分布式任务，大部分是短暂的，也有像marathon这种，长时间任务。

从marathon framework的角度将spark和marathon做一个对比

||基于的运行环境|提供抽象|一点不同|
|---|---|---|---|---|
|matahon|mesos|app、deployment等|marathon是一个long-live framework，可以管理app|
|spark|mesos/yarn等|rdd|spark 谈不上是一个framework，更像是一个framework的半成品（提供基础、抽象），结合用户代码作为一个framework执行，是否是long-live由用户对spark组件的选择决定||

从程序的执行上，spark和marathon听一样的，或者说分布式程序都挺一样的。

1. 首先有一个正在运行的集群管理器
2. spark、marathon等作为framework，配置集群管理器（yarn/mesos等）的地址（以便与集群管理其交互），作为一个独立的进程执行。假设我们现在在开发机上有一个可执行jar，我们执行这个jar，那么spark这个framework的主进程则运行在本机，通过与集群管理器安排framework的执行进程在集群中运行。

对于spark（其它framework也一样）来说，还可以将jar（及其依赖的jar）提交到集群管理器上，由集群管理器驱动framework主进程及执行进程的执行。

## spark stream 

spark stream 是微批处理。

spark，数据流进来，根据时间分隔成一小段，一小段做处理1、处理2、处理3。
storm，一个消息进来，一个消息做处理1、处理2、处理3



